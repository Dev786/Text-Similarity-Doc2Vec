{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1-cGCN4fzTg"
   },
   "source": [
    "# Doc2Vec Approach\n",
    "\n",
    "I am using google colab (as it is free and fast) to train the gensim doc2Vec model (It can also be trained locally but it might take more time and it can lead to some primary memory space constraints))\n",
    "\n",
    "**Importing the dataset from google drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "cisktox--47g",
    "outputId": "32b9d60e-30a3-4b1f-c21d-54109a1457d8"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import os\n",
    "# drive.mount('/content/drive/')\n",
    "# os.chdir('drive/My Drive/Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbkGZPP9gIkg"
   },
   "source": [
    "# **Assigning DatasetName**\n",
    "\n",
    "The same dataset name can be kept in local folder. The above code can be ignored, if you want to run this locally, just keep the file in the folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ikwAjFmKAvIM"
   },
   "outputs": [],
   "source": [
    "dataset_name = \"textSimilarity-dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIK8uO0XgQa6"
   },
   "source": [
    "# **Importing** Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "S8-rYBCgA1rF",
    "outputId": "8d25d7d1-78d6-48d2-db8a-6e6d2f84f350"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ranad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ranad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0b0ae44045ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import gensim\n",
    "import re\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r7ecWmE0hwiC"
   },
   "source": [
    "# **Loading data to dataframe**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cxGUuhQfA6K7"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dcrnMhs1h3zL"
   },
   "source": [
    "# **Function to preprocess**\n",
    "\n",
    "This function removes stopwords, punctuations and stems the words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oyFEVQ2tA86e"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text,remove_stopwords=True):\n",
    "  words = text.lower().split()\n",
    "  # remove stopwords based on flag\n",
    "  if remove_stopwords:\n",
    "    stops = set(stopwords.words('english'))\n",
    "    words = [w for w in words if w not in stops]\n",
    "  sentence = \" \".join(words)\n",
    "  sentence = re.sub(r\"[^A-Za-z0-9(),!.?\\'\\`]\", \" \", sentence)\n",
    "  sentence = re.sub(r\"\\'s\", \" 's \", sentence)\n",
    "  sentence = re.sub(r\"\\'ve\", \" 've \", sentence)\n",
    "  sentence = re.sub(r\"n\\'t\", \" 't \", sentence)\n",
    "  sentence = re.sub(r\"\\'re\", \" 're \", sentence)\n",
    "  sentence = re.sub(r\"\\'d\", \" 'd \", sentence)\n",
    "  sentence = re.sub(r\"\\'ll\", \" 'll \", sentence)\n",
    "  sentence = re.sub(r\",\", \" \", sentence)\n",
    "  sentence = re.sub(r\"\\.\", \" \", sentence)\n",
    "  sentence = re.sub(r\"!\", \" \", sentence)\n",
    "  sentence = re.sub(r\"\\(\", \" ( \", sentence)\n",
    "  sentence = re.sub(r\"\\)\", \" ) \", sentence)\n",
    "  sentence = re.sub(r\"\\?\", \" \", sentence)\n",
    "  sentence = re.sub(r\"\\s{2,}\", \" \", sentence)\n",
    "\n",
    "  words = sentence.split()\n",
    "  # Shorten words to their stems\n",
    "  stemmer = SnowballStemmer('english')\n",
    "  stemmed_words = [stemmer.stem(word) for word in words]\n",
    "  return \" \".join(stemmed_words)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhqMc96BimOE"
   },
   "source": [
    "**Preprocess all sentences and make a document array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bwIyPDI0Ep9J"
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for index,data in enumerate(df.values):\n",
    "  documents.append(preprocess_text(data[1],True))\n",
    "  documents.append(preprocess_text(data[2],True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "id7mpg1PjDUk"
   },
   "source": [
    "**Import Gensim module for document to vector and tagged documents to create dataformat for training the doc2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-l7uH4UiFUAH"
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2Vta59LGPSA"
   },
   "outputs": [],
   "source": [
    "# It creates word2Vec for all the words and uses tags as another input to neural network to generate a document vector\n",
    "tagged_documents = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(documents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iq9Y6uj0jmTN"
   },
   "source": [
    "# Training the model\n",
    "\n",
    "\n",
    "\n",
    "1.   You can define the epochs\n",
    "2.   the vector size is the size of the vector space required to represent a document\n",
    "3. alpha is the learning rate for the network\n",
    "4. Doc2Vec class is used to create doc2vec model, this model gives the vector representation for the sentence\n",
    "5. The model is saved so that it can be reused whenever required\n",
    "\n",
    "The learning rate is decayed to avoid large updates\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "fwI0scyRGY-8",
    "outputId": "6654bfcb-467c-4d1f-f4bc-f0025120960b"
   },
   "outputs": [],
   "source": [
    "def train_and_save(force_train=False,saved_model_name=\"d2v.model\"):\n",
    "  if os.path.exists(saved_model_name) and not force_train:\n",
    "    return\n",
    "  max_epochs = 10\n",
    "  vec_size = 100\n",
    "  alpha = 0.025\n",
    "\n",
    "  model = Doc2Vec(vector_size=vec_size,\n",
    "                  alpha=alpha, \n",
    "                  min_alpha=0.00025,\n",
    "                  min_count=1,\n",
    "                  dm =1)\n",
    "    \n",
    "  model.build_vocab(tagged_documents)\n",
    "\n",
    "  for epoch in range(max_epochs):\n",
    "      print('iteration {0}'.format(epoch))\n",
    "      model.train(tagged_documents,\n",
    "                  total_examples=model.corpus_count,\n",
    "                  epochs=model.iter)\n",
    "      # decrease the learning rate\n",
    "      model.alpha -= 0.0002\n",
    "      # fix the learning rate, no decay\n",
    "      model.min_alpha = model.alpha\n",
    "\n",
    "  model.save(\"saved_model_name\")\n",
    "  print(\"Model Saved\")\n",
    "\n",
    "train_and_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ixhi3tGkR3I"
   },
   "source": [
    "# Load the model to demonstrate reusability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "krMyaZq2Gp9S",
    "outputId": "6965289c-90f3-454e-95dd-b6040e43af7a"
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2IojhgRkjWh"
   },
   "source": [
    "# Scoring\n",
    "\n",
    "\n",
    "\n",
    "1.   Generate scores for each pair of sentences in dataset (based on some threshold value assign it label 0 (highly similar) or 1 (not similar)\n",
    "2.   put the score back to dataset\n",
    "3. **I have used 70% as threshold for similarity score**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "5DJeMfZzG5S7",
    "outputId": "062acb86-b79b-4b4a-b047-5e60e5553cd9"
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "threshold = 0.70\n",
    "# print(word_tokenize(documents[0]))\n",
    "# model.n_similarity(word_tokenize(documents[0]),word_tokenize(documents[i].split())\n",
    "for i in range(0,len(documents),2):\n",
    "  # print(i)\n",
    "  score = model.n_similarity(word_tokenize(documents[i]),word_tokenize(documents[i+1]))\n",
    "  scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OjQxtH8I4QbC"
   },
   "source": [
    "# Save the result to a new file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uVVVc6dd1SI"
   },
   "outputs": [],
   "source": [
    "data = {'Unique_ID':df['Unique_ID'],'Similarity_Score':scores}\n",
    "df_to_save = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5rQh1E22w43"
   },
   "outputs": [],
   "source": [
    "df_to_save.to_csv(\"final_text_similarity_scores.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text Similarity",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
